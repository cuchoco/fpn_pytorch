{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.ops.roi_pool import RoIPool\n",
    "from torchvision.ops.roi_align import RoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchROIPool(object):\n",
    "\n",
    "    def __init__(self, output_size, scaling_factor):\n",
    "        \"\"\"ROI max pooling works by dividing the hxw RoI window into an HxW grid of \n",
    "           approximately size h/H x w/W and then max-pooling the values in each\n",
    "           sub-window. Pooling is applied independently to each feature map channel.\n",
    "        \"\"\"\n",
    "        self.output_size = output_size\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def _roi_pool(self, features):\n",
    "        \"\"\"Given scaled and extracted features, do channel wise pooling\n",
    "        to return features of fixed size self.output_size, self.output_size\n",
    "\n",
    "        Args:\n",
    "            features (np.Array): scaled and extracted features of shape\n",
    "            num_channels, proposal_width, proposal_height\n",
    "        \"\"\"\n",
    "\n",
    "        num_channels, h, w = features.shape\n",
    "\n",
    "        w_stride = w/self.output_size\n",
    "        h_stride = h/self.output_size\n",
    "\n",
    "        res = torch.zeros((num_channels, self.output_size, self.output_size))\n",
    "        res_idx = torch.zeros((num_channels, self.output_size, self.output_size))\n",
    "        for i in range(self.output_size):\n",
    "            for j in range(self.output_size):\n",
    "                \n",
    "                # important to round the start and end, and then conver to int\n",
    "                w_start = int(np.floor(j*w_stride))\n",
    "                w_end = int(np.ceil((j+1)*w_stride))\n",
    "                h_start = int(np.floor(i*h_stride))\n",
    "                h_end = int(np.ceil((i+1)*h_stride))\n",
    "\n",
    "                # limiting start and end based on feature limits\n",
    "                w_start = min(max(w_start, 0), w)\n",
    "                w_end = min(max(w_end, 0), w)\n",
    "                h_start = min(max(h_start, 0), h)\n",
    "                h_end = min(max(h_end, 0), h)\n",
    "\n",
    "                patch = features[:, h_start: h_end, w_start: w_end]\n",
    "                max_val, max_idx = torch.max(patch.reshape(num_channels, -1), dim=1)\n",
    "                res[:, i, j] = max_val\n",
    "                res_idx[:, i, j] = max_idx\n",
    "\n",
    "        return res, res_idx\n",
    "\n",
    "    def __call__(self, feature_layer, proposals):\n",
    "        \"\"\"Given feature layers and a list of proposals, it returns pooled\n",
    "        respresentations of the proposals. Proposals are scaled by scaling factor\n",
    "        before pooling.\n",
    "\n",
    "        Args:\n",
    "            feature_layer (np.Array): Feature layer of size (num_channels, width,\n",
    "            height)\n",
    "            proposals (list of np.Array): Each element of the list represents a bounding\n",
    "            box as (x,y,w,h)\n",
    "\n",
    "        Returns:\n",
    "            np.Array: Shape len(proposals), channels, self.output_size, self.output_size\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_channels, _, _ = feature_layer.shape\n",
    "\n",
    "        # first scale proposals based on self.scaling factor \n",
    "        scaled_proposals = torch.zeros_like(proposals)\n",
    "\n",
    "        # the rounding by torch.ceil is important for ROI pool\n",
    "        scaled_proposals[:, 0] = torch.ceil(proposals[:, 0] * self.scaling_factor)\n",
    "        scaled_proposals[:, 1] = torch.ceil(proposals[:, 1] * self.scaling_factor)\n",
    "        scaled_proposals[:, 2] = torch.ceil(proposals[:, 2] * self.scaling_factor)\n",
    "        scaled_proposals[:, 3] = torch.ceil(proposals[:, 3] * self.scaling_factor)\n",
    "\n",
    "        res = torch.zeros((len(proposals), num_channels, self.output_size,\n",
    "                        self.output_size))\n",
    "        res_idx = torch.zeros((len(proposals), num_channels, self.output_size,\n",
    "                        self.output_size))\n",
    "        for idx in range(len(proposals)):\n",
    "            proposal = scaled_proposals[idx]\n",
    "            # adding 1 to include the end indices from proposal\n",
    "            extracted_feat = feature_layer[0, :, proposal[1].to(dtype=torch.int8):proposal[3].to(dtype=torch.int8)+1,\n",
    "                                           proposal[0].to(dtype=torch.int8):proposal[2].to(dtype=torch.int8)+1]\n",
    "            res[idx], res_idx[idx] = self._roi_pool(extracted_feat)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "\n",
    "# create feature layer, proposals and targets\n",
    "num_proposals = 10\n",
    "feat_layer = torch.randn(1, 64, 32, 32)\n",
    "\n",
    "proposals = torch.zeros((num_proposals, 4))\n",
    "proposals[:, 0] = torch.randint(0, 16, (num_proposals,))\n",
    "proposals[:, 1] = torch.randint(0, 16, (num_proposals,))\n",
    "proposals[:, 2] = torch.randint(16, 32, (num_proposals,))\n",
    "proposals[:, 3] = torch.randint(16, 32, (num_proposals,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_roi_pool = TorchROIPool(3, 2**-1)\n",
    "roi_pool1 = my_roi_pool(feat_layer, proposals)\n",
    "\n",
    "roi_pool = RoIPool(3, 2**-1)\n",
    "roi_pool2 = roi_pool(feat_layer, [proposals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4133, 2.7335, 1.6860],\n",
       "        [2.6391, 2.1293, 2.1205],\n",
       "        [3.0989, 2.8901, 2.8901]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_pool1[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 32])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_proposals = torch.zeros_like(proposals)\n",
    "\n",
    "# the rounding by torch.ceil is important for ROI pool\n",
    "scaled_proposals[:, 0] = torch.ceil(proposals[:, 0] * 2**-1)\n",
    "scaled_proposals[:, 1] = torch.ceil(proposals[:, 1] * 2**-1)\n",
    "scaled_proposals[:, 2] = torch.ceil(proposals[:, 2] * 2**-1)\n",
    "scaled_proposals[:, 3] = torch.ceil(proposals[:, 3] * 2**-1)\n",
    "\n",
    "# res = torch.zeros((len(proposals), num_channels, self.output_size,\n",
    "#                 self.output_size))\n",
    "# res_idx = torch.zeros((len(proposals), num_channels, self.output_size,\n",
    "#                 self.output_size))\n",
    "# for idx in range(len(proposals)):\n",
    "#     proposal = scaled_proposals[idx]\n",
    "#     # adding 1 to include the end indices from proposal\n",
    "#     extracted_feat = feature_layer[0, :, proposal[1].to(dtype=torch.int8):proposal[3].to(dtype=torch.int8)+1,\n",
    "#                                    proposal[0].to(dtype=torch.int8):proposal[2].to(dtype=torch.int8)+1]\n",
    "#     res[idx], res_idx[idx] = self._roi_pool(extracted_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  8., 15., 16.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_proposals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.zeros((10, 64, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15, 8])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_layer[0, :, 0:15 , 8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
